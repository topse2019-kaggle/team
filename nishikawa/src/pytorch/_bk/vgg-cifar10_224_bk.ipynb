{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各種ライブラリをインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUの利用可否"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data-loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "degress1 = 30\n",
    "degress2 = 60\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degress1),\n",
    "    transforms.RandomRotation(degress2),\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    transforms.RandomErasing(value='random')\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "num_workers = 10\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=data_transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "#trainset = torchvision.datasets.ImageFolder(root='./dataset/tiny-imagenet-200/train', transform=data_transform, target_transform=None, is_valid_file=None)\n",
    "#trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "#testset = torchvision.datasets.ImageFolder(root='./dataset/tiny-imagenet-200/test', transform=data_transform, target_transform=None, is_valid_file=None)\n",
    "#testloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_dict = {\n",
    "    \"train\" : trainloader,\n",
    "    \"val\" : testloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_features = 7 * 7 * 512\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        # Block1\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.norm1 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.drop1 = nn.Dropout(p=0.5)\n",
    "        # Block2\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.norm2 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.drop2 = nn.Dropout(p=0.5)\n",
    "        # Block3\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv7 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.norm3 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.drop3 = nn.Dropout(p=0.5)\n",
    "        # Block4\n",
    "        self.conv8 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv9 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv10 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.norm4 = nn.BatchNorm2d(512)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.drop4 = nn.Dropout(p=0.5)\n",
    "        # Block5\n",
    "        self.conv11 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv12 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv13 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.norm5 = nn.BatchNorm2d(512)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.drop5 = nn.Dropout(p=0.5)\n",
    "        # fc\n",
    "        self.fc1 = nn.Linear(in_features=flatten_features, out_features=4096)\n",
    "        self.fc2 = nn.Linear(in_features=4096, out_features=4096)\n",
    "        self.drop6 = nn.Dropout(p=0.5)\n",
    "        self.fc3 = nn.Linear(in_features=4096, out_features=10)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Block1\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.norm1(x)\n",
    "        x = self.pool1(x)\n",
    "        #x = self.drop1(x)\n",
    "        # Block2\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.norm2(x)\n",
    "        x = self.pool2(x)\n",
    "        #x = self.drop2(x)\n",
    "        # Block3\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = self.norm3(x)\n",
    "        x = self.pool3(x)\n",
    "        #x = self.drop3(x)\n",
    "        # Block4\n",
    "        x = F.relu(self.conv8(x))\n",
    "        x = F.relu(self.conv9(x))\n",
    "        x = F.relu(self.conv10(x))\n",
    "        x = self.norm4(x)\n",
    "        x = self.pool4(x)\n",
    "        #x = self.drop4(x)\n",
    "        # Block5\n",
    "        x = F.relu(self.conv11(x))\n",
    "        x = F.relu(self.conv12(x))\n",
    "        x = F.relu(self.conv13(x))\n",
    "        x = self.norm5(x)\n",
    "        x = self.pool5(x)\n",
    "        x = self.drop5(x)\n",
    "        # fc\n",
    "        x = self.avgpool(x)\n",
    "        #x = x.view(-1, flatten_features)\n",
    "        #x = x.view(x.size(0),-1)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop6(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(input=self.fc3(x), dim=0)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = VGG16()\n",
    "net = net.to(device)\n",
    "net = torch.nn.DataParallel(net)\n",
    "#net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimazerの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "----------\n",
      "Epoch 1/100 2020-01-03 13:59:14.112414\n",
      "val Loss: 2.3026 Acc: 0.0944\n",
      "----------\n",
      "Epoch 2/100 2020-01-03 13:59:38.746353\n",
      "train Loss: 2.2897 Acc: 0.1568\n",
      "val Loss: 2.2551 Acc: 0.2432\n",
      "----------\n",
      "Epoch 3/100 2020-01-03 14:05:15.031490\n",
      "train Loss: 2.2424 Acc: 0.2424\n",
      "val Loss: 2.2144 Acc: 0.2971\n",
      "----------\n",
      "Epoch 4/100 2020-01-03 14:10:56.606542\n",
      "train Loss: 2.2141 Acc: 0.2908\n",
      "val Loss: 2.2001 Acc: 0.3188\n",
      "----------\n",
      "Epoch 5/100 2020-01-03 14:16:37.473825\n",
      "train Loss: 2.1991 Acc: 0.3122\n",
      "val Loss: 2.1868 Acc: 0.3382\n",
      "----------\n",
      "Epoch 6/100 2020-01-03 14:22:18.617433\n",
      "train Loss: 2.1906 Acc: 0.3307\n",
      "val Loss: 2.1774 Acc: 0.3456\n",
      "----------\n",
      "Epoch 7/100 2020-01-03 14:27:59.982329\n",
      "train Loss: 2.1817 Acc: 0.3440\n",
      "val Loss: 2.1685 Acc: 0.3705\n",
      "----------\n",
      "Epoch 8/100 2020-01-03 14:33:40.297132\n",
      "train Loss: 2.1766 Acc: 0.3513\n",
      "val Loss: 2.1718 Acc: 0.3684\n",
      "----------\n",
      "Epoch 9/100 2020-01-03 14:39:20.593538\n",
      "train Loss: 2.1711 Acc: 0.3584\n",
      "val Loss: 2.1553 Acc: 0.3959\n",
      "----------\n",
      "Epoch 10/100 2020-01-03 14:45:00.950047\n",
      "train Loss: 2.1678 Acc: 0.3614\n",
      "val Loss: 2.1534 Acc: 0.3940\n",
      "----------\n",
      "Epoch 11/100 2020-01-03 14:50:41.746287\n",
      "train Loss: 2.1646 Acc: 0.3699\n",
      "val Loss: 2.1440 Acc: 0.4110\n",
      "----------\n",
      "Epoch 12/100 2020-01-03 14:56:22.773679\n",
      "train Loss: 2.1602 Acc: 0.3771\n",
      "val Loss: 2.1433 Acc: 0.4133\n",
      "----------\n",
      "Epoch 13/100 2020-01-03 15:02:03.810116\n",
      "train Loss: 2.1571 Acc: 0.3790\n",
      "val Loss: 2.1393 Acc: 0.4061\n",
      "----------\n",
      "Epoch 14/100 2020-01-03 15:07:44.935733\n",
      "train Loss: 2.1545 Acc: 0.3829\n",
      "val Loss: 2.1410 Acc: 0.4109\n",
      "----------\n",
      "Epoch 15/100 2020-01-03 15:13:26.129826\n",
      "train Loss: 2.1508 Acc: 0.3886\n",
      "val Loss: 2.1301 Acc: 0.4277\n",
      "----------\n",
      "Epoch 16/100 2020-01-03 15:19:07.345859\n",
      "train Loss: 2.1501 Acc: 0.3910\n",
      "val Loss: 2.1323 Acc: 0.4245\n",
      "----------\n",
      "Epoch 17/100 2020-01-03 15:24:48.580867\n",
      "train Loss: 2.1460 Acc: 0.3971\n",
      "val Loss: 2.1296 Acc: 0.4392\n",
      "----------\n",
      "Epoch 18/100 2020-01-03 15:30:29.869492\n",
      "train Loss: 2.1446 Acc: 0.4008\n",
      "val Loss: 2.1211 Acc: 0.4397\n",
      "----------\n",
      "Epoch 19/100 2020-01-03 15:36:12.438184\n",
      "train Loss: 2.1421 Acc: 0.4038\n",
      "val Loss: 2.1197 Acc: 0.4433\n",
      "----------\n",
      "Epoch 20/100 2020-01-03 15:41:54.327645\n",
      "train Loss: 2.1402 Acc: 0.4076\n",
      "val Loss: 2.1161 Acc: 0.4582\n",
      "----------\n",
      "Epoch 21/100 2020-01-03 15:47:35.292392\n",
      "train Loss: 2.1367 Acc: 0.4102\n",
      "val Loss: 2.1157 Acc: 0.4553\n",
      "----------\n",
      "Epoch 22/100 2020-01-03 15:53:16.074444\n",
      "train Loss: 2.1347 Acc: 0.4165\n",
      "val Loss: 2.1158 Acc: 0.4538\n",
      "----------\n",
      "Epoch 23/100 2020-01-03 15:58:56.492712\n",
      "train Loss: 2.1334 Acc: 0.4166\n",
      "val Loss: 2.1127 Acc: 0.4552\n",
      "----------\n",
      "Epoch 24/100 2020-01-03 16:04:36.725004\n",
      "train Loss: 2.1315 Acc: 0.4205\n",
      "val Loss: 2.1126 Acc: 0.4675\n",
      "----------\n",
      "Epoch 25/100 2020-01-03 16:10:16.879307\n",
      "train Loss: 2.1296 Acc: 0.4233\n",
      "val Loss: 2.1054 Acc: 0.4655\n",
      "----------\n",
      "Epoch 26/100 2020-01-03 16:15:56.824827\n",
      "train Loss: 2.1282 Acc: 0.4263\n",
      "val Loss: 2.1086 Acc: 0.4665\n",
      "----------\n",
      "Epoch 27/100 2020-01-03 16:21:36.451439\n",
      "train Loss: 2.1245 Acc: 0.4337\n",
      "val Loss: 2.1045 Acc: 0.4724\n",
      "----------\n",
      "Epoch 28/100 2020-01-03 16:27:15.913725\n",
      "train Loss: 2.1225 Acc: 0.4343\n",
      "val Loss: 2.1025 Acc: 0.4743\n",
      "----------\n",
      "Epoch 29/100 2020-01-03 16:32:55.563281\n",
      "train Loss: 2.1231 Acc: 0.4341\n",
      "val Loss: 2.1021 Acc: 0.4740\n",
      "----------\n",
      "Epoch 30/100 2020-01-03 16:38:35.503345\n",
      "train Loss: 2.1215 Acc: 0.4402\n",
      "val Loss: 2.0985 Acc: 0.4788\n",
      "----------\n",
      "Epoch 31/100 2020-01-03 16:44:15.498560\n",
      "train Loss: 2.1185 Acc: 0.4418\n",
      "val Loss: 2.0932 Acc: 0.4885\n",
      "----------\n",
      "Epoch 32/100 2020-01-03 16:49:55.535173\n",
      "train Loss: 2.1166 Acc: 0.4446\n",
      "val Loss: 2.0923 Acc: 0.4871\n",
      "----------\n",
      "Epoch 33/100 2020-01-03 16:55:35.566518\n",
      "train Loss: 2.1164 Acc: 0.4476\n",
      "val Loss: 2.0915 Acc: 0.5028\n",
      "----------\n",
      "Epoch 34/100 2020-01-03 17:01:15.665477\n",
      "train Loss: 2.1137 Acc: 0.4522\n",
      "val Loss: 2.0920 Acc: 0.4968\n",
      "----------\n",
      "Epoch 35/100 2020-01-03 17:06:55.557458\n",
      "train Loss: 2.1132 Acc: 0.4530\n",
      "val Loss: 2.0869 Acc: 0.4975\n",
      "----------\n",
      "Epoch 36/100 2020-01-03 17:12:35.448585\n",
      "train Loss: 2.1131 Acc: 0.4548\n",
      "val Loss: 2.0858 Acc: 0.5050\n",
      "----------\n",
      "Epoch 37/100 2020-01-03 17:18:15.319307\n",
      "train Loss: 2.1109 Acc: 0.4590\n",
      "val Loss: 2.0871 Acc: 0.4923\n",
      "----------\n",
      "Epoch 38/100 2020-01-03 17:23:58.061279\n",
      "train Loss: 2.1077 Acc: 0.4639\n",
      "val Loss: 2.0807 Acc: 0.5146\n",
      "----------\n",
      "Epoch 39/100 2020-01-03 17:29:42.156489\n",
      "train Loss: 2.1078 Acc: 0.4645\n",
      "val Loss: 2.0819 Acc: 0.5165\n",
      "----------\n",
      "Epoch 40/100 2020-01-03 17:35:23.244991\n",
      "train Loss: 2.1043 Acc: 0.4685\n",
      "val Loss: 2.0792 Acc: 0.5224\n",
      "----------\n",
      "Epoch 41/100 2020-01-03 17:41:03.268224\n",
      "train Loss: 2.1028 Acc: 0.4725\n",
      "val Loss: 2.0768 Acc: 0.5185\n",
      "----------\n",
      "Epoch 42/100 2020-01-03 17:46:43.293551\n",
      "train Loss: 2.1029 Acc: 0.4729\n",
      "val Loss: 2.0797 Acc: 0.5146\n",
      "----------\n",
      "Epoch 43/100 2020-01-03 17:52:25.447326\n",
      "train Loss: 2.1023 Acc: 0.4729\n",
      "val Loss: 2.0788 Acc: 0.5146\n",
      "----------\n",
      "Epoch 44/100 2020-01-03 17:58:13.668307\n",
      "train Loss: 2.1015 Acc: 0.4757\n",
      "val Loss: 2.0769 Acc: 0.5169\n",
      "----------\n",
      "Epoch 45/100 2020-01-03 18:04:00.345235\n",
      "train Loss: 2.0982 Acc: 0.4797\n",
      "val Loss: 2.0762 Acc: 0.5123\n",
      "----------\n",
      "Epoch 46/100 2020-01-03 18:09:47.109683\n",
      "train Loss: 2.0984 Acc: 0.4806\n",
      "val Loss: 2.0709 Acc: 0.5302\n",
      "----------\n",
      "Epoch 47/100 2020-01-03 18:15:32.891772\n",
      "train Loss: 2.0972 Acc: 0.4819\n",
      "val Loss: 2.0712 Acc: 0.5241\n",
      "----------\n",
      "Epoch 48/100 2020-01-03 18:21:16.081006\n",
      "train Loss: 2.0967 Acc: 0.4829\n",
      "val Loss: 2.0720 Acc: 0.5228\n",
      "----------\n",
      "Epoch 49/100 2020-01-03 18:27:02.065509\n",
      "train Loss: 2.0962 Acc: 0.4864\n",
      "val Loss: 2.0684 Acc: 0.5304\n",
      "----------\n",
      "Epoch 50/100 2020-01-03 18:32:48.093342\n",
      "train Loss: 2.0961 Acc: 0.4856\n",
      "val Loss: 2.0709 Acc: 0.5273\n",
      "----------\n",
      "Epoch 51/100 2020-01-03 18:38:28.920012\n",
      "train Loss: 2.0939 Acc: 0.4878\n",
      "val Loss: 2.0669 Acc: 0.5337\n",
      "----------\n",
      "Epoch 52/100 2020-01-03 18:44:09.577551\n",
      "train Loss: 2.0913 Acc: 0.4912\n",
      "val Loss: 2.0659 Acc: 0.5450\n",
      "----------\n",
      "Epoch 53/100 2020-01-03 18:49:50.008984\n",
      "train Loss: 2.0917 Acc: 0.4903\n",
      "val Loss: 2.0644 Acc: 0.5418\n",
      "----------\n",
      "Epoch 54/100 2020-01-03 18:55:29.760767\n",
      "train Loss: 2.0914 Acc: 0.4915\n",
      "val Loss: 2.0611 Acc: 0.5455\n",
      "----------\n",
      "Epoch 55/100 2020-01-03 19:01:09.538855\n",
      "train Loss: 2.0907 Acc: 0.4957\n",
      "val Loss: 2.0639 Acc: 0.5458\n",
      "----------\n",
      "Epoch 56/100 2020-01-03 19:06:49.211158\n",
      "train Loss: 2.0901 Acc: 0.4968\n",
      "val Loss: 2.0641 Acc: 0.5508\n",
      "----------\n",
      "Epoch 57/100 2020-01-03 19:12:28.751470\n",
      "train Loss: 2.0887 Acc: 0.5019\n",
      "val Loss: 2.0577 Acc: 0.5538\n",
      "----------\n",
      "Epoch 58/100 2020-01-03 19:18:08.160246\n",
      "train Loss: 2.0864 Acc: 0.4988\n",
      "val Loss: 2.0598 Acc: 0.5544\n",
      "----------\n",
      "Epoch 59/100 2020-01-03 19:23:47.672437\n",
      "train Loss: 2.0871 Acc: 0.5039\n",
      "val Loss: 2.0556 Acc: 0.5653\n",
      "----------\n",
      "Epoch 60/100 2020-01-03 19:29:27.174363\n",
      "train Loss: 2.0841 Acc: 0.5064\n",
      "val Loss: 2.0566 Acc: 0.5552\n",
      "----------\n",
      "Epoch 61/100 2020-01-03 19:35:06.754925\n",
      "train Loss: 2.0844 Acc: 0.5074\n",
      "val Loss: 2.0585 Acc: 0.5492\n",
      "----------\n",
      "Epoch 62/100 2020-01-03 19:40:46.344154\n",
      "train Loss: 2.0837 Acc: 0.5078\n",
      "val Loss: 2.0562 Acc: 0.5613\n",
      "----------\n",
      "Epoch 63/100 2020-01-03 19:46:25.994412\n",
      "train Loss: 2.0838 Acc: 0.5093\n",
      "val Loss: 2.0562 Acc: 0.5573\n",
      "----------\n",
      "Epoch 64/100 2020-01-03 19:52:05.500167\n",
      "train Loss: 2.0838 Acc: 0.5122\n",
      "val Loss: 2.0563 Acc: 0.5581\n",
      "----------\n",
      "Epoch 65/100 2020-01-03 19:57:44.971440\n",
      "train Loss: 2.0825 Acc: 0.5106\n",
      "val Loss: 2.0558 Acc: 0.5576\n",
      "----------\n",
      "Epoch 66/100 2020-01-03 20:03:24.508382\n",
      "train Loss: 2.0832 Acc: 0.5078\n",
      "val Loss: 2.0541 Acc: 0.5680\n",
      "----------\n",
      "Epoch 67/100 2020-01-03 20:09:03.937647\n",
      "train Loss: 2.0823 Acc: 0.5138\n",
      "val Loss: 2.0508 Acc: 0.5766\n",
      "----------\n",
      "Epoch 68/100 2020-01-03 20:14:43.309491\n",
      "train Loss: 2.0813 Acc: 0.5169\n",
      "val Loss: 2.0568 Acc: 0.5605\n",
      "----------\n",
      "Epoch 69/100 2020-01-03 20:20:22.784559\n",
      "train Loss: 2.0792 Acc: 0.5159\n",
      "val Loss: 2.0535 Acc: 0.5693\n",
      "----------\n",
      "Epoch 70/100 2020-01-03 20:26:03.682689\n",
      "train Loss: 2.0782 Acc: 0.5202\n",
      "val Loss: 2.0546 Acc: 0.5696\n",
      "----------\n",
      "Epoch 71/100 2020-01-03 20:31:43.120445\n",
      "train Loss: 2.0790 Acc: 0.5181\n",
      "val Loss: 2.0497 Acc: 0.5764\n",
      "----------\n",
      "Epoch 72/100 2020-01-03 20:37:22.683491\n",
      "train Loss: 2.0763 Acc: 0.5224\n",
      "val Loss: 2.0484 Acc: 0.5724\n",
      "----------\n",
      "Epoch 73/100 2020-01-03 20:43:02.127733\n",
      "train Loss: 2.0781 Acc: 0.5209\n",
      "val Loss: 2.0547 Acc: 0.5610\n",
      "----------\n",
      "Epoch 74/100 2020-01-03 20:48:41.533166\n",
      "train Loss: 2.0757 Acc: 0.5214\n",
      "val Loss: 2.0515 Acc: 0.5656\n",
      "----------\n",
      "Epoch 75/100 2020-01-03 20:54:24.467298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.0758 Acc: 0.5276\n",
      "val Loss: 2.0473 Acc: 0.5792\n",
      "----------\n",
      "Epoch 76/100 2020-01-03 21:00:10.929434\n",
      "train Loss: 2.0761 Acc: 0.5208\n",
      "val Loss: 2.0494 Acc: 0.5739\n",
      "----------\n",
      "Epoch 77/100 2020-01-03 21:05:56.356986\n",
      "train Loss: 2.0748 Acc: 0.5285\n",
      "val Loss: 2.0482 Acc: 0.5819\n",
      "----------\n",
      "Epoch 78/100 2020-01-03 21:11:37.376489\n",
      "train Loss: 2.0731 Acc: 0.5306\n",
      "val Loss: 2.0467 Acc: 0.5914\n",
      "----------\n",
      "Epoch 79/100 2020-01-03 21:17:20.581548\n",
      "train Loss: 2.0721 Acc: 0.5329\n",
      "val Loss: 2.0477 Acc: 0.5775\n",
      "----------\n",
      "Epoch 80/100 2020-01-03 21:23:04.137666\n",
      "train Loss: 2.0708 Acc: 0.5283\n",
      "val Loss: 2.0451 Acc: 0.5887\n",
      "----------\n",
      "Epoch 81/100 2020-01-03 21:28:51.152023\n",
      "train Loss: 2.0724 Acc: 0.5292\n",
      "val Loss: 2.0481 Acc: 0.5741\n",
      "----------\n",
      "Epoch 82/100 2020-01-03 21:34:33.786789\n",
      "train Loss: 2.0728 Acc: 0.5298\n",
      "val Loss: 2.0450 Acc: 0.5834\n",
      "----------\n",
      "Epoch 83/100 2020-01-03 21:40:15.423830\n",
      "train Loss: 2.0716 Acc: 0.5314\n",
      "val Loss: 2.0451 Acc: 0.5780\n",
      "----------\n",
      "Epoch 84/100 2020-01-03 21:46:00.185410\n",
      "train Loss: 2.0708 Acc: 0.5337\n",
      "val Loss: 2.0429 Acc: 0.5827\n",
      "----------\n",
      "Epoch 85/100 2020-01-03 21:51:43.097056\n",
      "train Loss: 2.0717 Acc: 0.5343\n",
      "val Loss: 2.0453 Acc: 0.5899\n",
      "----------\n",
      "Epoch 86/100 2020-01-03 21:57:26.837339\n",
      "train Loss: 2.0714 Acc: 0.5338\n",
      "val Loss: 2.0436 Acc: 0.5902\n",
      "----------\n",
      "Epoch 87/100 2020-01-03 22:03:08.522744\n",
      "train Loss: 2.0690 Acc: 0.5391\n",
      "val Loss: 2.0478 Acc: 0.5760\n",
      "----------\n",
      "Epoch 88/100 2020-01-03 22:08:51.484151\n",
      "train Loss: 2.0697 Acc: 0.5347\n",
      "val Loss: 2.0436 Acc: 0.5882\n",
      "----------\n",
      "Epoch 89/100 2020-01-03 22:14:35.950477\n",
      "train Loss: 2.0688 Acc: 0.5399\n",
      "val Loss: 2.0422 Acc: 0.5967\n",
      "----------\n",
      "Epoch 90/100 2020-01-03 22:20:19.588645\n",
      "train Loss: 2.0681 Acc: 0.5398\n",
      "val Loss: 2.0428 Acc: 0.5979\n",
      "----------\n",
      "Epoch 91/100 2020-01-03 22:26:03.207067\n",
      "train Loss: 2.0667 Acc: 0.5434\n",
      "val Loss: 2.0386 Acc: 0.6012\n",
      "----------\n",
      "Epoch 92/100 2020-01-03 22:31:45.073956\n",
      "train Loss: 2.0652 Acc: 0.5455\n",
      "val Loss: 2.0391 Acc: 0.5975\n",
      "----------\n",
      "Epoch 93/100 2020-01-03 22:37:25.574991\n",
      "train Loss: 2.0678 Acc: 0.5439\n",
      "val Loss: 2.0424 Acc: 0.5967\n",
      "----------\n",
      "Epoch 94/100 2020-01-03 22:43:05.355361\n",
      "train Loss: 2.0664 Acc: 0.5458\n",
      "val Loss: 2.0389 Acc: 0.6001\n",
      "----------\n",
      "Epoch 95/100 2020-01-03 22:48:44.377742\n",
      "train Loss: 2.0651 Acc: 0.5429\n",
      "val Loss: 2.0411 Acc: 0.5922\n",
      "----------\n",
      "Epoch 96/100 2020-01-03 22:54:23.376868\n",
      "train Loss: 2.0645 Acc: 0.5478\n",
      "val Loss: 2.0411 Acc: 0.5921\n",
      "----------\n",
      "Epoch 97/100 2020-01-03 23:00:02.204283\n",
      "train Loss: 2.0640 Acc: 0.5441\n",
      "val Loss: 2.0407 Acc: 0.5931\n",
      "----------\n",
      "Epoch 98/100 2020-01-03 23:05:41.045938\n",
      "train Loss: 2.0638 Acc: 0.5477\n",
      "val Loss: 2.0416 Acc: 0.5939\n",
      "----------\n",
      "Epoch 99/100 2020-01-03 23:11:19.912821\n",
      "train Loss: 2.0654 Acc: 0.5456\n",
      "val Loss: 2.0412 Acc: 0.5914\n",
      "----------\n",
      "Epoch 100/100 2020-01-03 23:16:58.790123\n",
      "train Loss: 2.0635 Acc: 0.5522\n"
     ]
    }
   ],
   "source": [
    "print(len(trainloader))\n",
    "\n",
    "epoch_count = 100\n",
    "#check_point = 100\n",
    "\n",
    "for epoch in range(epoch_count):  # loop over the dataset multiple times\n",
    "    print('----------')\n",
    "    print('Epoch {}/{} {}'.format(epoch+1, epoch_count, datetime.datetime.now()))\n",
    "\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            net.train()\n",
    "        else:\n",
    "            net.eval()\n",
    "      \n",
    "        epoch_loss = 0.0\n",
    "        epoch_corrects = 0\n",
    "\n",
    "        if (epoch == 0) and (phase == 'train'):\n",
    "            continue\n",
    "      \n",
    "#      for inputs, labels in tqdm(dataloaders_dict[phase]):\n",
    "        for i, data in enumerate(dataloaders_dict[phase], 0):\n",
    "  \n",
    "            inputs, labels = data\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item() * inputs.size(0)\n",
    "                epoch_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "        epoch_acc = epoch_corrects.double() / len(dataloaders_dict[phase].dataset)\n",
    "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "        \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        #inputs = torch.reshape(inputs, (-1, 3, image_size, image_size))\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #scheduler.setp()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % check_point == 0:    # print every 2000 mini-batches\n",
    "            print(datetime.datetime.now())\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 1000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータによる検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "#plt.imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習結果を保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './vgg-cifar_net_tmp.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
