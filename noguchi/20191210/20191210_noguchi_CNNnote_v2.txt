・Googleコラボ環境を想定

・環境が違うと挙動が変わるなどの場合、TensorFlowやKerasのバージョンを確認する。
　→バージョンによって挙動が異なる場合がある。

・pytorchでGPUを使うときは、GPUにモデルを置かないと使われない。
　→.cudaメソッドを使う。
・Kerasの場合
　ドライバが分かれてる。TensorFlowとTensorFlowGPUがある。後者を入れるとGPUが使われる。

・kerasは自動的にGPUを使うようになっている模様。
　→Cloud上で運用する場合などには課金条件に注意。

・写経時にはとりあえず下記(Keras Documentation)でKerasのメソッド名を検索する。
　→<https://keras.io/ja/>

・畳み込みフィルタのfilterは別々のフィルタ32枚ではない。
　→32レイヤを持った1枚のフィルタだと考える。

・32レイヤだとすると、1回目のフィルタ当てた結果に対して2枚目のフィルタを当てる...ではなく、
　同時に32個の何らかの特徴を抽出しようとしている。
　→箱x32(32次元output)

・フィルタ(kernel)の中身の初期値はどうなっているか。
　→なんでもいい。乱数など。
　→フィルタの中身そのものも機械学習で学習していく。（勾配降下法による最適化の対象）
　→誤差関数が最小になるような適切なフィルタが自動的に得られる。
　※フィルタのウェイトの初期化の方法は性能に効いてくる。
　　イチから学習する場合は気にすると良くなる。
　　学習がうまくいかない場合はいじってみるとよくなるかもしれない。
　　アーキを作った後調整する場合→model.parameters(違うかも)で重みを指定。
　　fine tuningするには必要な操作。（重みを更新しないという操作）

・フィルタの数を増やすとき、慣例的にkernelのサイズを小さくする
　→より細かい部分の特徴を抽出しようとしている。
　画像が小さくなったとき

・MNISTのようにデータセットが基本的で簡単な場合、複雑なネットワークを適用してしまうと、
　学習する力が強すぎて過学習を起こし画像を記憶してしまう。

・Augumentationは疑似的な学習データの水増し。
　回転処理などは実行しても問題ないか検討する必要あり。





