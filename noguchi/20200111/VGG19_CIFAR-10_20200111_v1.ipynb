{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VGG19_CIFAR-10_20200111_v1.ipynb","provenance":[],"collapsed_sections":["cuexTdXYw7zV"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"M5-RwpkFQw0y","colab_type":"text"},"source":["参考：\n","\n","[1] CIFAR-10 and CIFAR-100 datasets\n","\n","(<http://www.cs.toronto.edu/~kriz/cifar.html>)\n","\n","[2] Kaggle Kernel: CNN Architectures : VGG, ResNet, Inception + TL\n","\n","(<https://www.kaggle.com/shivamb/cnn-architectures-vgg-resnet-inception-tl>)\n","\n","[3] cifar10とKerasを使ってCNN(Convolutional Neural Network)を実装してみる\n","\n","(<https://qiita.com/God_KonaBanana/items/10fa8bb58cdd1dbd2e59>)\n","\n","[4] DeepLearning：VGG16をkerasで実装した\n","\n","(<http://blog.neko-ni-naritai.com/entry/2018/04/07/115504>)\n"]},{"cell_type":"markdown","metadata":{"id":"kQqw5L1-4IEC","colab_type":"text"},"source":["# 1.0 Introduction"]},{"cell_type":"markdown","metadata":{"id":"pCp-wNs74Y0m","colab_type":"text"},"source":["① データセットとして何を選んだか：CIFAR-10\n","\n","② 実装面で注意が必要だった点を該当箇所の近辺に：随所に記載\n","\n","③ 日々の作業の進捗を、末尾に、変更履歴のような形で(他のメンバが参照する際に、前回参照時からの差分をすぐに理解できるように)：\n","* 20200111: 初版\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LU27nz5QOPnG","colab_type":"text"},"source":["## 1.1 ライブラリをインポート"]},{"cell_type":"code","metadata":{"id":"vCFbnPK5vVuE","colab_type":"code","outputId":"c4fdcd5c-204b-44f6-9a1a-977049a162ae","executionInfo":{"status":"ok","timestamp":1578664813143,"user_tz":-540,"elapsed":2597,"user":{"displayName":"Naoki Noguchi","photoUrl":"https://lh5.googleusercontent.com/-aKeN7lyJx04/AAAAAAAAAAI/AAAAAAAAAC4/sHc_5wyq7wE/s64/photo.jpg","userId":"11922372683711516364"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# 各種ライブラリをインポートする。\n","%tensorflow_version 1.x\n","from keras.datasets import cifar10\n","import matplotlib.pyplot as plt\n","import numpy as np"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"HqZCIOKPgR7S","colab_type":"text"},"source":["# 2.0 Data preparation\n","\n"]},{"cell_type":"markdown","metadata":{"id":"awIM4so0SEeI","colab_type":"text"},"source":["## 2.1 Load data"]},{"cell_type":"code","metadata":{"id":"YAXdQFIjYOCo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"4a987ea8-5ee5-4a56-be46-540e42acb165","executionInfo":{"status":"ok","timestamp":1578664819433,"user_tz":-540,"elapsed":7271,"user":{"displayName":"Naoki Noguchi","photoUrl":"https://lh5.googleusercontent.com/-aKeN7lyJx04/AAAAAAAAAAI/AAAAAAAAAC4/sHc_5wyq7wE/s64/photo.jpg","userId":"11922372683711516364"}}},"source":["# CIFAR-10のデータセットをロードする。\n","(x_train,y_train),(x_test,y_test)=cifar10.load_data()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 4s 0us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cuexTdXYw7zV","colab_type":"text"},"source":["## 2.1.1 Check data"]},{"cell_type":"code","metadata":{"id":"SIoUznv2VtIQ","colab_type":"code","colab":{}},"source":["# 画像データの次元を確認する。\n","print(\"x_train:\")\n","print(\"Dimension:\", x_train.ndim)\n","print(\"Size:\", x_train.shape[0], x_train.shape[1], x_train.shape[2], x_train.shape[3])\n","print(\"y_train:\")\n","print(\"Dimension:\", y_train.ndim)\n","print(\"Size:\", y_train.shape[0], y_train.shape[1])\n","print(\"\")\n","print(\"x_test:\")\n","print(\"Dimension:\", x_test.ndim)\n","print(\"Size:\", x_test.shape[0], x_test.shape[1], x_test.shape[2], x_test.shape[3])\n","print(\"y_test:\")\n","print(\"Dimension:\", y_test.ndim)\n","print(\"Size:\", y_test.shape[0], y_test.shape[1])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fp3KXzF3OorP","colab_type":"code","colab":{}},"source":["# 簡単に画像データを確認する。\n","# 確認する画像の番号tmp_iを指定する。\n","tmp_i = 2\n","# データを確認する。\n","print(tmp_i, \"番目の画像の先頭の1ピクセルのRGB値:\", x_train[tmp_i][0][0])\n","print(tmp_i, \"番目の画像の正解ラベル値:\", y_train[tmp_i])\n","# 画像を表示する。\n","plt.figure(figsize=(2,2))\n","cifar_img = plt.subplot(1,1,1)\n","plt.imshow(x_train[tmp_i])\n","plt.tick_params(labelbottom=False)\n","plt.tick_params(labelleft=False)\n","plt.title(y_train[tmp_i])\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I6ELY3IYH7D1","colab_type":"code","colab":{}},"source":["# CIFAR-10の画像を確認する。ランダムに選択した5x5枚の画像を表示する。\n","plt.figure(figsize=(10,10))\n","for i in range(25):\n","  rand_num = np.random.randint(0,50000)\n","  cifar_img = plt.subplot(5,5,i+1)\n","  plt.imshow(x_train[rand_num])\n","  # x軸の目盛りを消す。\n","  # (「='off'」だと消えなかったので「=False」に修正)\n","  plt.tick_params(labelbottom=False)\n","  # y軸の目盛りを消す。\n","  # (「='off'」だと消えなかったので「=False」に修正)\n","  plt.tick_params(labelleft=False)\n","  # 正解ラベルを表示する。\n","  plt.title(y_train[rand_num])\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YKgi7F4ZJ50n","colab_type":"text"},"source":["0. airplane(飛行機)\n","1. automobile(自動車)\n","2. bird(鳥)\n","3. cat(猫)\n","4. deer(鹿)\n","5. dog(犬)\n","6. frog(蛙)\n","7. horse(馬)\n","8. ship(船)\n","9. truck(トラック)"]},{"cell_type":"markdown","metadata":{"id":"E41an-lruXh2","colab_type":"text"},"source":["##2.2 Check for null and missing values"]},{"cell_type":"markdown","metadata":{"id":"PjfxOQtpe25u","colab_type":"text"},"source":["## 2.3 Normalization"]},{"cell_type":"code","metadata":{"id":"Pbs0tzjWaLFF","colab_type":"code","colab":{}},"source":["# 正規化前のデータを確認する。\n","print(tmp_i, \"番目の画像の先頭の1ピクセルのRGB値:\", x_train[tmp_i][0][0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P9E3RAyOOCQs","colab_type":"code","colab":{}},"source":["# 画像データが0-1の値域になるように正規化する。\n","x_train = x_train.astype('float32')/255.0\n","x_test = x_test.astype('float32')/255.0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hjOuL62GZJBg","colab_type":"code","colab":{}},"source":["# 正規化後のデータを確認する。\n","print(tmp_i, \"番目の画像の先頭の1ピクセルのRGB値:\", x_train[tmp_i][0][0])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RX7FcKPPfohB","colab_type":"text"},"source":["## 2.4 Reshape"]},{"cell_type":"markdown","metadata":{"id":"ulz_sakgjQk2","colab_type":"text"},"source":["## 2.5 Label encoding"]},{"cell_type":"code","metadata":{"id":"HlolaGzrZ1ae","colab_type":"code","colab":{}},"source":["# 変換前のデータを確認する。\n","print(\"y_train:\")\n","print(\"Dimension:\", y_train.ndim)\n","print(\"Size:\", y_train.shape[0], y_train.shape[1])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S9kPzTqUOcW_","colab_type":"code","colab":{}},"source":["# 正解ラベルをOne-Hot表現に変換する。\n","from keras.utils import np_utils\n","y_train = np_utils.to_categorical(y_train,10)\n","y_test = np_utils.to_categorical(y_test,10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Htk0LmnuZ3HX","colab_type":"code","colab":{}},"source":["# 変換後のデータを確認する。\n","print(\"y_train:\")\n","print(\"Dimension:\", y_train.ndim)\n","print(\"Size:\", y_train.shape[0], y_train.shape[1])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3d2xw4pyZr4f","colab_type":"code","colab":{}},"source":["# ラベルを確認する。\n","print(tmp_i, \"番目の画像の正解ラベル値:\", y_train[tmp_i])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nkPefDVSjV9e","colab_type":"text"},"source":["## 2.6 Split training and valdiation set"]},{"cell_type":"markdown","metadata":{"id":"D5HAx3prmTwj","colab_type":"text"},"source":["# 3.0 CNN"]},{"cell_type":"markdown","metadata":{"id":"JQgvBBh9mf32","colab_type":"text"},"source":["## 3.1 Define the model"]},{"cell_type":"code","metadata":{"id":"9vPw7vkNddww","colab_type":"code","outputId":"3ef99abc-3a28-455e-895f-56562576fb3a","executionInfo":{"status":"ok","timestamp":1578664827869,"user_tz":-540,"elapsed":1953,"user":{"displayName":"Naoki Noguchi","photoUrl":"https://lh5.googleusercontent.com/-aKeN7lyJx04/AAAAAAAAAAI/AAAAAAAAAC4/sHc_5wyq7wE/s64/photo.jpg","userId":"11922372683711516364"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["# モデル定義に必要なライブラリをインポートする。\n","from keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Lambda\n","from keras.models import Model\n","import tensorflow as tf\n","\n","# CIFAR-10の画像サイズは32x32x3。\n","inputs = Input(shape=(32,32,3), name='input_image')\n","inputs_tmp = inputs\n","\n","########## 以下のリサイズ(32x32->224x224)を有効にすると、CNNへの入力サイズが本来のVGGのサイズ(224x224)になる。 ##########\n","########## ただし、演算が重くなり、batch_sizeが大きいとクラッシュするようになる。                              ##########\n","########## 32x32のまま実行する場合はコメントアウトする。                                                       ##########\n","# VGG19のinput sizeは224x224であるため、最初に画像サイズを拡大する。\n","#inputs_tmp = Lambda(lambda image: tf.image.resize_images(image, (224,224)))(inputs)\n","\n","# VGG19モデルを定義する。KerasライブラリのVGG19にconvを3つ足しただけのモデル。\n","conv1  = Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='block1_conv1')(inputs_tmp)\n","conv2  = Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='block1_conv2')(conv1)\n","pool1  = MaxPooling2D((2, 2), name='block1_pool1')(conv2)\n","\n","conv3  = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='block2_conv1')(pool1)\n","conv4  = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='block2_conv2')(conv3)\n","pool2  = MaxPooling2D((2, 2), name='block2_pool1')(conv4)\n","\n","conv5  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='block3_conv1')(pool2)\n","conv6  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='block3_conv2')(conv5)\n","conv7  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='block3_conv3')(conv6)\n","conv8  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='block3_conv4')(conv7)\n","pool3  = MaxPooling2D((2, 2), name='block3_pool1')(conv8)\n","\n","conv9  = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='block4_conv1')(pool3)\n","conv10 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='block4_conv2')(conv9)\n","conv11 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='block4_conv3')(conv10)\n","conv12 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='block4_conv4')(conv11)\n","pool4  = MaxPooling2D((2, 2), name='block4_pool1')(conv12)\n","\n","conv13 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='block5_conv1')(pool4)\n","conv14 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='block5_conv2')(conv13)\n","conv15 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='block5_conv3')(conv14)\n","conv16 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='block5_conv4')(conv15)\n","pool5  = MaxPooling2D((2, 2), name='block5_pool1')(conv16)\n","\n","flat   = Flatten(name='flatten')(pool5)\n","dense1 = Dense(4096, activation=\"relu\", name='fc1')(flat)\n","dense2 = Dense(4096, activation=\"relu\", name='fc2')(dense1)\n","output = Dense(10, activation=\"softmax\", name='predictions')(dense2) # ラベル数に応じて変更。\n","\n","model = Model(inputs=inputs, outputs=output)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iYp-ngfLkPcz","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zQ_vLnAinasj","colab_type":"text"},"source":["## 3.2 Set the optimizer and annealer"]},{"cell_type":"code","metadata":{"id":"RJ7Ixodumxym","colab_type":"code","colab":{}},"source":["########## OptimizerにRMSpropを指定すると学習が上手くいかなくなる。 ##########\n","# OptimizerにRMSpropを指定。\n","#from keras import optimizers\n","#optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XNAKtD6ppEtb","colab_type":"code","colab":{}},"source":["# OptimizerにSGDを指定。\n","from keras import optimizers\n","optimizer = optimizers.SGD(lr=0.01, momentum=0.9, decay=5e-4)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tSqiTBp8nDW6","colab_type":"code","outputId":"a63c2bad-675c-4867-8fcf-8d9cdf962fa5","executionInfo":{"status":"ok","timestamp":1578664837342,"user_tz":-540,"elapsed":620,"user":{"displayName":"Naoki Noguchi","photoUrl":"https://lh5.googleusercontent.com/-aKeN7lyJx04/AAAAAAAAAAI/AAAAAAAAAC4/sHc_5wyq7wE/s64/photo.jpg","userId":"11922372683711516364"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["# モデルをコンパイルする。\n","model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"],"execution_count":9,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iTWOHXcznsHg","colab_type":"code","outputId":"cfc5d7ec-5103-429b-a9e3-7b15fedc1598","executionInfo":{"status":"ok","timestamp":1578664838852,"user_tz":-540,"elapsed":751,"user":{"displayName":"Naoki Noguchi","photoUrl":"https://lh5.googleusercontent.com/-aKeN7lyJx04/AAAAAAAAAAI/AAAAAAAAAC4/sHc_5wyq7wE/s64/photo.jpg","userId":"11922372683711516364"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["# アニーラー設定。3-epochの間、正解率が向上しなかった場合には学習率を0.5倍にする。\n","from keras.callbacks import ReduceLROnPlateau\n","rlop = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=3, verbose=1, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0.00001)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1335: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n","  warnings.warn('`epsilon` argument is deprecated and '\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"9TPizfTuoxwp","colab_type":"code","colab":{}},"source":["# エポック数とミニバッチサイズを設定する。\n","epochs = 50  # VGGの原論文では=74とのこと。\n","batch_size = 256"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HQd9EsAtRCsO","colab_type":"text"},"source":["## 3.3 Data augmentation"]},{"cell_type":"code","metadata":{"id":"00L0XRKppHcx","colab_type":"code","outputId":"d2e7d9a2-845f-4525-ddbe-c5031034ea2c","executionInfo":{"status":"ok","timestamp":1578685430314,"user_tz":-540,"elapsed":20588249,"user":{"displayName":"Naoki Noguchi","photoUrl":"https://lh5.googleusercontent.com/-aKeN7lyJx04/AAAAAAAAAAI/AAAAAAAAAC4/sHc_5wyq7wE/s64/photo.jpg","userId":"11922372683711516364"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# 実行時間を計測する。\n","%%time\n","\n","# Data augumentationをしない場合のFIT。\n","history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[rlop], validation_data=(x_test, y_test))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","Train on 50000 samples, validate on 10000 samples\n","Epoch 1/50\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","50000/50000 [==============================] - 435s 9ms/step - loss: 2.2987 - acc: 0.1139 - val_loss: 2.3027 - val_acc: 0.1000\n","Epoch 2/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 2.1841 - acc: 0.1748 - val_loss: 1.8192 - val_acc: 0.3441\n","Epoch 3/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 1.6988 - acc: 0.3895 - val_loss: 1.5169 - val_acc: 0.4616\n","Epoch 4/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 1.3780 - acc: 0.5143 - val_loss: 1.2096 - val_acc: 0.5729\n","Epoch 5/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 1.0699 - acc: 0.6251 - val_loss: 1.0417 - val_acc: 0.6299\n","Epoch 6/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 0.8170 - acc: 0.7180 - val_loss: 0.9284 - val_acc: 0.6827\n","Epoch 7/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 0.5672 - acc: 0.8035 - val_loss: 0.9239 - val_acc: 0.6888\n","Epoch 8/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 0.3366 - acc: 0.8830 - val_loss: 1.0334 - val_acc: 0.6966\n","Epoch 9/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 0.1972 - acc: 0.9328 - val_loss: 1.4443 - val_acc: 0.6894\n","Epoch 10/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 0.1114 - acc: 0.9623 - val_loss: 1.5543 - val_acc: 0.7049\n","Epoch 11/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 0.0752 - acc: 0.9755 - val_loss: 1.7667 - val_acc: 0.6947\n","Epoch 12/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 0.0484 - acc: 0.9845 - val_loss: 1.7810 - val_acc: 0.7054\n","Epoch 13/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 0.0308 - acc: 0.9900 - val_loss: 1.8318 - val_acc: 0.7088\n","Epoch 14/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 0.0234 - acc: 0.9924 - val_loss: 1.8978 - val_acc: 0.7080\n","Epoch 15/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 0.0153 - acc: 0.9952 - val_loss: 1.8078 - val_acc: 0.7204\n","Epoch 16/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 0.0108 - acc: 0.9961 - val_loss: 2.1111 - val_acc: 0.7104\n","Epoch 17/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 0.0111 - acc: 0.9963 - val_loss: 1.9291 - val_acc: 0.7187\n","Epoch 18/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 0.0063 - acc: 0.9979 - val_loss: 2.1500 - val_acc: 0.7245\n","Epoch 19/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 0.0035 - acc: 0.9991 - val_loss: 2.2607 - val_acc: 0.7214\n","Epoch 20/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 2.3438 - val_acc: 0.7265\n","Epoch 21/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 6.3379e-04 - acc: 0.9998 - val_loss: 2.4387 - val_acc: 0.7287\n","Epoch 22/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 2.9680e-04 - acc: 0.9999 - val_loss: 2.5346 - val_acc: 0.7237\n","Epoch 23/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 1.8923e-05 - acc: 1.0000 - val_loss: 2.5459 - val_acc: 0.7242\n","Epoch 24/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 8.8228e-06 - acc: 1.0000 - val_loss: 2.5618 - val_acc: 0.7244\n","\n","Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n","Epoch 25/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 7.4142e-06 - acc: 1.0000 - val_loss: 2.5687 - val_acc: 0.7245\n","Epoch 26/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 6.8679e-06 - acc: 1.0000 - val_loss: 2.5747 - val_acc: 0.7247\n","Epoch 27/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 6.4222e-06 - acc: 1.0000 - val_loss: 2.5803 - val_acc: 0.7251\n","\n","Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n","Epoch 28/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 6.1015e-06 - acc: 1.0000 - val_loss: 2.5826 - val_acc: 0.7251\n","Epoch 29/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 5.9513e-06 - acc: 1.0000 - val_loss: 2.5847 - val_acc: 0.7250\n","Epoch 30/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 5.8141e-06 - acc: 1.0000 - val_loss: 2.5868 - val_acc: 0.7250\n","\n","Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n","Epoch 31/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 5.7105e-06 - acc: 1.0000 - val_loss: 2.5875 - val_acc: 0.7250\n","Epoch 32/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 5.6648e-06 - acc: 1.0000 - val_loss: 2.5881 - val_acc: 0.7251\n","Epoch 33/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 5.6215e-06 - acc: 1.0000 - val_loss: 2.5888 - val_acc: 0.7251\n","\n","Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n","Epoch 34/50\n","50000/50000 [==============================] - 412s 8ms/step - loss: 5.5888e-06 - acc: 1.0000 - val_loss: 2.5889 - val_acc: 0.7251\n","Epoch 35/50\n","50000/50000 [==============================] - 412s 8ms/step - loss: 5.5765e-06 - acc: 1.0000 - val_loss: 2.5891 - val_acc: 0.7251\n","Epoch 36/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 5.5649e-06 - acc: 1.0000 - val_loss: 2.5892 - val_acc: 0.7251\n","\n","Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n","Epoch 37/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 5.5558e-06 - acc: 1.0000 - val_loss: 2.5893 - val_acc: 0.7251\n","Epoch 38/50\n","50000/50000 [==============================] - 412s 8ms/step - loss: 5.5528e-06 - acc: 1.0000 - val_loss: 2.5893 - val_acc: 0.7251\n","Epoch 39/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 5.5499e-06 - acc: 1.0000 - val_loss: 2.5894 - val_acc: 0.7251\n","\n","Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n","Epoch 40/50\n","50000/50000 [==============================] - 412s 8ms/step - loss: 5.5477e-06 - acc: 1.0000 - val_loss: 2.5894 - val_acc: 0.7251\n","Epoch 41/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 5.5470e-06 - acc: 1.0000 - val_loss: 2.5894 - val_acc: 0.7251\n","Epoch 42/50\n","50000/50000 [==============================] - 412s 8ms/step - loss: 5.5463e-06 - acc: 1.0000 - val_loss: 2.5894 - val_acc: 0.7251\n","\n","Epoch 00042: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n","Epoch 43/50\n","50000/50000 [==============================] - 412s 8ms/step - loss: 5.5458e-06 - acc: 1.0000 - val_loss: 2.5894 - val_acc: 0.7251\n","Epoch 44/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 5.5456e-06 - acc: 1.0000 - val_loss: 2.5894 - val_acc: 0.7251\n","Epoch 45/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 5.5455e-06 - acc: 1.0000 - val_loss: 2.5894 - val_acc: 0.7251\n","\n","Epoch 00045: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n","Epoch 46/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 5.5453e-06 - acc: 1.0000 - val_loss: 2.5894 - val_acc: 0.7251\n","Epoch 47/50\n","50000/50000 [==============================] - 412s 8ms/step - loss: 5.5453e-06 - acc: 1.0000 - val_loss: 2.5894 - val_acc: 0.7251\n","Epoch 48/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 5.5453e-06 - acc: 1.0000 - val_loss: 2.5894 - val_acc: 0.7251\n","\n","Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n","Epoch 49/50\n","50000/50000 [==============================] - 412s 8ms/step - loss: 5.5452e-06 - acc: 1.0000 - val_loss: 2.5894 - val_acc: 0.7251\n","Epoch 50/50\n","50000/50000 [==============================] - 411s 8ms/step - loss: 5.5452e-06 - acc: 1.0000 - val_loss: 2.5894 - val_acc: 0.7251\n","CPU times: user 2h 22min 27s, sys: 1h 25min 5s, total: 3h 47min 32s\n","Wall time: 5h 43min 7s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Iy0qPJBE6mjd","colab_type":"code","colab":{}},"source":["# 実行時間を計測する。\n","%%time\n","\n","# Data augmentationをする場合のFIT。\n","from keras.preprocessing.image import ImageDataGenerator\n","datagen = ImageDataGenerator(\n","        featurewise_center=False,             # set input mean to 0 over the dataset\n","        samplewise_center=False,              # set each sample mean to 0\n","        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n","        samplewise_std_normalization=False,   # divide each input by its std\n","        zca_whitening=False,                  # apply ZCA whitening\n","        rotation_range=10,                    # randomly rotate images in the range (degrees, 0 to 180)\n","        zoom_range = 0.1,                     # Randomly zoom image \n","        width_shift_range=0.1,                # randomly shift images horizontally (fraction of total width)\n","        height_shift_range=0.1,               # randomly shift images vertically (fraction of total height)\n","        horizontal_flip=False,                # randomly flip images\n","        vertical_flip=False)                  # randomly flip images\n","datagen.fit(x_train)\n","\n","history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n","                              epochs=epochs, verbose=1, callbacks=[rlop], validation_data=(x_test,y_test),  \n","                              steps_per_epoch=x_train.shape[0] // batch_size)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uxd1WiQQTcCp","colab_type":"text"},"source":["## [Additional] 3.4 Export Trained Model"]},{"cell_type":"code","metadata":{"id":"kuLWPp5781Lo","colab_type":"code","colab":{}},"source":["# Google Driveをmountする。\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# 学習済みのモデルをGoogle Driveにエクスポートしておく。\n","model.save('/content/drive/My Drive/TopSE/ソフトウェア開発実践演習/tmp/TrainedModel.hd5')\n","!ls -lh '/content/drive/My Drive/TopSE/ソフトウェア開発実践演習/tmp/TrainedModel.hd5'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PF9f4sfiTQCy","colab_type":"text"},"source":["# 4. Evaluate the model"]},{"cell_type":"markdown","metadata":{"id":"xeoQpIUFTSk_","colab_type":"text"},"source":["## 4.1 Training and validation curves"]},{"cell_type":"code","metadata":{"id":"z2GkHFn4rYop","colab_type":"code","colab":{}},"source":["# 学習時/確認時の誤差関数および正解率を表示する。\n","fig, ax = plt.subplots(2,1)\n","ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n","ax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\", axes=ax[0])\n","legend = ax[0].legend(loc='best', shadow=True)\n","\n","ax[1].plot(history.history['acc'], color='b', label=\"Training accuracy\")\n","ax[1].plot(history.history['val_acc'], color='r',label=\"Validation accuracy\")\n","legend = ax[1].legend(loc='best', shadow=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-cvguf9jThIY","colab_type":"text"},"source":["## 4.2 Confusion matrix"]},{"cell_type":"code","metadata":{"id":"JOVTtF8EhsXw","colab_type":"code","colab":{}},"source":["# Look at confusion matrix\n","from sklearn.metrics import confusion_matrix\n","import itertools\n","def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, cm[i, j],\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","\n","# Predict the values from the validation dataset\n","Y_pred = model.predict(x_test)\n","# Convert predictions classes to one hot vectors \n","Y_pred_classes = np.argmax(Y_pred, axis=1) \n","# Convert validation observations to one hot vectors\n","Y_true = np.argmax(y_test,axis=1) \n","# compute the confusion matrix\n","confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n","# plot the confusion matrix\n","plot_confusion_matrix(confusion_mtx, classes=range(10)) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pjv-HnxFLLkw","colab_type":"code","colab":{}},"source":["# Errors are difference between predicted labels and true labels\n","errors = (Y_pred_classes - Y_true != 0)\n","\n","Y_pred_classes_errors = Y_pred_classes[errors]\n","Y_pred_errors = Y_pred[errors]\n","Y_true_errors = Y_true[errors]\n","X_val_errors = x_test[errors]\n","\n","def display_errors(errors_index, img_errors, pred_errors, obs_errors):\n","    \"\"\" This function shows 10 images with their predicted and real labels\"\"\"\n","    n = 0\n","    nrows = 2\n","    ncols = 5\n","    fig, ax = plt.subplots(nrows, ncols, sharex=True, sharey=True)\n","    for row in range(nrows):\n","        for col in range(ncols):\n","            error = errors_index[n]\n","            ax[row,col].imshow((img_errors[error]).reshape((32,32,3)))\n","            ax[row,col].set_title(\"Pdct:{}\\nTrue:{}\".format(pred_errors[error], obs_errors[error]))\n","            n += 1\n","\n","# Probabilities of the wrong predicted numbers\n","Y_pred_errors_prob = np.max(Y_pred_errors, axis = 1)\n","\n","# Predicted probabilities of the true values in the error set\n","true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n","\n","# Difference between the probability of the predicted label and the true label\n","delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n","\n","# Sorted list of the delta prob errors\n","sorted_dela_errors = np.argsort(delta_pred_true_errors)\n","\n","# Top 10 errors \n","most_important_errors = sorted_dela_errors[-10:]\n","\n","# Show the top 10 errors\n","display_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sXfweo8-Z_s2","colab_type":"text"},"source":["0. airplane(飛行機)\n","1. automobile(自動車)\n","2. bird(鳥)\n","3. cat(猫)\n","4. deer(鹿)\n","5. dog(犬)\n","6. frog(蛙)\n","7. horse(馬)\n","8. ship(船)\n","9. truck(トラック)"]}]}